{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOninuksBbmVjfoHDn8GW6u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RevathyRamalingam/machineLearning/blob/main/TensorFlowPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SmZ_lidwhHsR"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.xception import Xception,preprocess_input,decode_predictions\n",
        "from tensorflow.keras.preprocessing.image import load_img,ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/alexeygrigorev/clothing-dataset-small.git\n",
        "\n",
        "path='./clothing-dataset-small'\n",
        "train_gen=ImageDataGenerator(preprocessing_function =preprocess_input)\n",
        "train_ds=train_gen.flow_from_directory(f'{path}/train',target_size=(150,150),batch_size=32)\n",
        "\n",
        "X,y = next(train_ds)\n",
        "val_gen=ImageDataGenerator(preprocessing_function =preprocess_input)\n",
        "val_ds=val_gen.flow_from_directory(f'{path}/validation',target_size=(150,150),batch_size=32,shuffle=False)\n",
        "\n",
        "base_model =Xception(\n",
        "    weights='imagenet',\n",
        "    include_top=False, #dont include dense layers simply use convolutional layers -the filters\n",
        "    input_shape=(150,150,3)\n",
        "\n",
        ")\n",
        "base_model.trainable = False# freeze convolutional layers bottom layers\n",
        "inputs = keras.Input(shape=(150,150,3))\n",
        "base = base_model(inputs,training=False)\n",
        "vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
        "outputs=keras.layers.Dense(10)(vectors)\n",
        "model = keras.Model(inputs,outputs)\n",
        "preds = model.predict(X)\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer,loss=loss,metrics=['accuracy'])\n",
        "model.fit(train_ds, epochs =10,validation_data=val_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1igjmgVuI9t_",
        "outputId": "51d45853-1816-433e-c648-d4adad20d1f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'clothing-dataset-small'...\n",
            "remote: Enumerating objects: 3839, done.\u001b[K\n",
            "remote: Counting objects: 100% (400/400), done.\u001b[K\n",
            "remote: Compressing objects: 100% (400/400), done.\u001b[K\n",
            "remote: Total 3839 (delta 9), reused 385 (delta 0), pack-reused 3439 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3839/3839), 100.58 MiB | 47.29 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n",
            "Found 3068 images belonging to 10 classes.\n",
            "Found 341 images belonging to 10 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 15s/step\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 367ms/step - accuracy: 0.5941 - loss: 1.6159 - val_accuracy: 0.7243 - val_loss: 0.8740\n",
            "Epoch 2/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 75ms/step - accuracy: 0.8371 - loss: 0.5043 - val_accuracy: 0.7830 - val_loss: 0.8592\n",
            "Epoch 3/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 69ms/step - accuracy: 0.8586 - loss: 0.4608 - val_accuracy: 0.7595 - val_loss: 1.0027\n",
            "Epoch 4/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 77ms/step - accuracy: 0.9250 - loss: 0.2306 - val_accuracy: 0.8387 - val_loss: 0.8195\n",
            "Epoch 5/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 76ms/step - accuracy: 0.9547 - loss: 0.1130 - val_accuracy: 0.8065 - val_loss: 0.8657\n",
            "Epoch 6/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 73ms/step - accuracy: 0.9744 - loss: 0.0786 - val_accuracy: 0.7947 - val_loss: 0.8806\n",
            "Epoch 7/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 74ms/step - accuracy: 0.9865 - loss: 0.0579 - val_accuracy: 0.8065 - val_loss: 0.9241\n",
            "Epoch 8/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 72ms/step - accuracy: 0.9788 - loss: 0.0647 - val_accuracy: 0.7654 - val_loss: 1.0777\n",
            "Epoch 9/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 78ms/step - accuracy: 0.9855 - loss: 0.0515 - val_accuracy: 0.7771 - val_loss: 1.0793\n",
            "Epoch 10/10\n",
            "\u001b[1m96/96\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 77ms/step - accuracy: 0.9730 - loss: 0.0778 - val_accuracy: 0.7918 - val_loss: 1.0328\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a7eb393c7a0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}